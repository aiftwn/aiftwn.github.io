List of features a deep reinforcement learning chess neural network will need.
-----------
 - Positions of all pieces.
 - Side to move.
 - Complete list of all possible moves that could be played in the current turn, as well as the list of all moves the opponent could play in the next turn. (How deep can this go? This scales up fast. There are *4,897,256* total possible positions a chess board could have after white plays its fifth move. This number should be much bigger during middle game.)
 - How do I encode information such that the network can understand not to leave a piece hanging? It is essential practice to move your pieces to a square protected by another one of your pieces. Maybe have a map
 - Output neurons should be of two sets - one is location on chess board, the other will be the piece to move to that location. Why? I think this might lead to good attacking strategies, and since it is going to play against itself, it will also result in better defensive strategies. How? well. You often wish a certain piece of yours was at a particular position but you find you cannot achieve in the next move because it'll take more than one or two moves to get your piece there. Thinking about location on the chess board independently, I think, will give the network a chance to 'think' about good moves - and it will learn how to implement them through training.
 - How do I handle draws? Do I penalise drawing in a winning position and award drawing in losing position?
 - Piece based evaluation of game will not be considered. Ex Evaluation parameters like Average Centipawn loss, to me, are useless. The objective of the game is to checkmate the king and not to capture pieces. They might go along hand in hand but they are in the end mutually exclusive endeavours. So, I need to award the network for checkmating the king and not to capture pieces. It'll soon learn capturing pieces makes that job easier, I do however want to see if it manages to learn checkmating without keeping capturing pieces in high priority. Which network architecture can implement awarding a single occurrence in a game without a continuous evaluation during the game?
 - In my opinion a true position evaluation would be a binary state - black or white if both sides were to be played by the same engine with the same computing power. Not really relevant to the neural network architecture since continuous position evaluation will not be implemented, but it is relevant to my interests.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE2NDgzMDIxOTNdfQ==
-->